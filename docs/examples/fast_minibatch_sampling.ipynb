{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast minibatch sampling\n",
    "\n",
    "In this example, we show how to create a fast minibatch generator, which is typically used in Machine Learning to feed a training routine.\n",
    "It is not the intent of SeqTools to supplant specialized libraries such as tensorflow's [data module](https://www.tensorflow.org/guide/datasets) or [torch.utils.Dataset](https://pytorch.org/docs/stable/data.html), but these might lack simplicity and flexibility for certain usages.\n",
    "Besides, it is absolutly possible to use seqtools to provide the inputs for these modules.\n",
    "\n",
    "**Note**: As a general guideline, special care should be taken when using worker based functions along with these libraries.\n",
    "User are advised to become familiar with the behaviour of Python [threads](https://docs.python.org/3/library/threading.html) and [processes](https://docs.python.org/3/library/multiprocessing.html) before using them.\n",
    "\n",
    "## Data samples\n",
    "\n",
    "For this example we consider a set of (X, y) data samples composed of a real vector observation and an integer label.\n",
    "Since it is common practice to store these samples by large groups into a few binary dump files, the following script generates such files to mock a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "\n",
    "workdir = tempfile.TemporaryDirectory()\n",
    "os.chdir(workdir.name)\n",
    "\n",
    "n_samples = 18000\n",
    "n_classes = 10\n",
    "sample_shape = (248,)\n",
    "chunk_size = 5000\n",
    "\n",
    "# generate reference class centers\n",
    "means = np.random.randn(n_classes, *sample_shape) * 3\n",
    "\n",
    "# generate random class labels\n",
    "labels = np.random.randint(n_classes, size=n_samples)\n",
    "np.save('labels.npy', labels)\n",
    "\n",
    "# generate noisy samples\n",
    "n_chunks = n_samples // chunk_size + (1 if n_samples % chunk_size > 0 else 0)\n",
    "for i in range(n_chunks):\n",
    "    n = min((i + 1) * chunk_size, n_samples) - i * chunk_size\n",
    "    chunk_file = \"data_{:02d}.npy\".format(i)\n",
    "    data = means[labels[i * chunk_size:i * chunk_size + n]] \\\n",
    "        + np.random.randn(n, *sample_shape) * 0.1\n",
    "    np.save(chunk_file, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "\n",
    "Now begins the actual data loading.\n",
    "Assuming the dataset is too big to fit in memory, data samples are not loaded but mapped into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import seqtools\n",
    "\n",
    "labels = np.load(\"labels.npy\")\n",
    "\n",
    "data_files = sorted(f for f in os.listdir() if f.startswith('data_'))\n",
    "data_chunks = [np.load(f, mmap_mode='r') for f in data_files]\n",
    "data = seqtools.concatenate(data_chunks)\n",
    "\n",
    "assert len(data) == n_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Concatenate is easy to memorize and does the job, but for that particular case we could also use `data = seqtools.unbatch(data_chunks)` since all of our data chunks (except for the last one) have the same size.\n",
    "\n",
    "Let's now assemble the samples with their labels to facilitate manipulation and split the dataset between training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = seqtools.collate([data, labels])\n",
    "train_dataset = dataset[:-10000]\n",
    "test_dataset = dataset[-10000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now write a simple random minibatch sampler and pass it to `seqtools.load_buffers` to start generating samples using multiple background workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assemble_minibatch(samples):\n",
    "    \"\"\"Assembles a bunch of samples into a minibatch.\"\"\"\n",
    "    batch_data = np.stack([data for data, _ in samples])\n",
    "    batch_labels = np.stack([label for _, label in samples])\n",
    "    return batch_data, batch_labels\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def sample_minibatch():\n",
    "    subset = np.random.choice(len(dataset), batch_size)\n",
    "    samples = list(seqtools.gather(dataset, subset))\n",
    "    return assemble_minibatch(samples)\n",
    "\n",
    "\n",
    "minibatch_iter = seqtools.load_buffers(sample_minibatch, max_cached=10, nworkers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`minibatch_iter` simply yields minibatches indefinitely by repeatedly calling `sample_minibatch` and puts the results into buffers which are returned at each iteration.\n",
    "\n",
    "*Please, note that the buffer slots are reused cyclicly so their content is overwritten across iterations.*\n",
    "\n",
    "\n",
    "## Training\n",
    "\n",
    "With the minibatches ready to be used, we create a Gaussian Naive Bayes model and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(4000):\n",
    "    X, y = next(minibatch_iter)\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "    \n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without `seqtools.load_buffers` to prefetech the minibatches, the training procedure must wait for its input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "classes = np.arange(n_classes)\n",
    "\n",
    "t1 = time.time()\n",
    "for _ in range(4000):\n",
    "    X, y = sample_minibatch()  # load the data traditionally\n",
    "    model.partial_fit(X, y, classes=classes)\n",
    "\n",
    "t2 = time.time()\n",
    "print(\"training took {:.0f}s\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "\n",
    "For completeness, we evaluate the accuracy of the results on the testing data.\n",
    "Assuming the testing dataset is also too big, the evaluation also proceeeds by small chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_chunks = seqtools.batch(test_dataset, 64, collate_fn=assemble_minibatch)\n",
    "\n",
    "predictions = []\n",
    "targets = []\n",
    "\n",
    "t1 = time.time()\n",
    "for X, y in testing_chunks:\n",
    "    predictions.extend(model.predict(X))\n",
    "    targets.extend(y)\n",
    "\n",
    "accuracy = np.mean(np.array(predictions) == np.array(targets))\n",
    "print(\"Accuracy: {:.0f}%\".format(accuracy * 100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
